# Transformers 

In this repository, I implement the Transformer and apply it to various seq2seq language taks: spelling and grammar corrector, paraphrase generation and text summarization. The goal is to explore how the following modifications influence performance on each task:

- **Static Positional Encoding** vs **Learned Positional Encoding** 
- **Label Smoothing**
- **Word-level Encoding** vs **Character-level Encoding** vs **Byte Part Encoding**
- **Greedy Decoding** vs **Beam Search**
